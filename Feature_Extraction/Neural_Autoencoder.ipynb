{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211f9b31-590d-45e7-bb2e-a404581d1bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import auraloss\n",
    "import collections\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import plotly.graph_objects as go\n",
    "import pretty_midi\n",
    "import pytorch_lightning as pl\n",
    "import pywt\n",
    "import random\n",
    "import scipy.signal\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae7c64b-5c45-4d34-a716-7c00c5b8a608",
   "metadata": {},
   "source": [
    "# Set Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ab3b5d-0603-4d2c-8836-d44b5d081625",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 3407\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151194b5-0e2c-4acc-a4da-75e6b8fc8806",
   "metadata": {},
   "source": [
    "# caching audio\n",
    "\n",
    "1) load audio from hard drive\n",
    "2) reduce sample rate from 44100 to 32000 (this is primarily to save ram)\n",
    "3) store in data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220ddba1-d2cc-499e-ab2c-2e6fe40d67a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/rendered_audio/rendered_audio/\"\n",
    "files = os.listdir(path)\n",
    "all_scenes = {}\n",
    "counter = 0\n",
    "transform = torchaudio.transforms.Resample(44100, 32000)\n",
    "\n",
    "for file in tqdm(files):\n",
    "    if \".flac\" in file:\n",
    "        try:\n",
    "            full_path = path + file\n",
    "            audio, _ = torchaudio.load(full_path)\n",
    "            audio = transform(audio)\n",
    "            all_scenes[counter] = {'path':full_path, 'audio':audio, 'sr':32000}\n",
    "            counter += 1\n",
    "        except Exception as e:\n",
    "            print(\"error\", e)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3062e287-d23c-47c9-bf13-9ae99d8ddec3",
   "metadata": {},
   "source": [
    "# torch data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec7c32c-5a0d-4881-95ab-cf005c705fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataGenerator(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        audio_tensor = sample['audio']\n",
    "        return audio_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c329d688-fe57-4ac0-a102-e8b15d5723af",
   "metadata": {},
   "source": [
    "# torch data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3213266d-45d3-457c-b4b1-e06a85a6c32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data, batch_size=32, num_workers=0, persistent_workers=False, shuffle=True):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.persistent_workers=persistent_workers\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Split your data here if necessary, e.g., into train, validation, test\n",
    "        self.dataset = AudioDataGenerator(self.data)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset, batch_size=self.batch_size, shuffle=self.shuffle, num_workers = self.num_workers, persistent_workers=self.persistent_workers)\n",
    "\n",
    "    # Implement val_dataloader() and test_dataloader() if you have validation and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f914862-114c-4c94-b9fa-f1a11526fa73",
   "metadata": {},
   "source": [
    "# Teh Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41369077-b581-423a-a907-19d316ebd890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        #losses\n",
    "        self.loss_fn_1 = auraloss.freq.RandomResolutionSTFTLoss(\n",
    "                    sample_rate=32000,\n",
    "                    device=\"cuda\"\n",
    "                )\n",
    "        self.loss_fn_2 = auraloss.time.SISDRLoss()\n",
    "        self.loss_fn_3 = torch.nn.L1Loss()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc_conv1 = nn.Conv1d(1, 8, kernel_size=33, stride=4, padding=16)\n",
    "        self.enc_conv2 = nn.Conv1d(8, 16, kernel_size=17, stride=4, padding=8)\n",
    "        self.enc_conv3 = nn.Conv1d(16, 32, kernel_size=9, stride=2, padding=4)\n",
    "        self.enc_conv4 = nn.Conv1d(32, 64, kernel_size=9, stride=2, padding=4)\n",
    "        self.enc_conv5 = nn.Conv1d(64,128, kernel_size=9, stride=2, padding=4)\n",
    "        self.enc_conv6 = nn.Conv1d(128, 256, kernel_size=9, stride=2, padding=4)\n",
    "        self.enc_conv7 = nn.Conv1d(256, 512, kernel_size=9, stride=2, padding=4)\n",
    "        self.enc_conv8 = nn.Conv1d(512, 1024, kernel_size=9, stride=2, padding=4)\n",
    "        \n",
    "        # Decoder\n",
    "        self.dec_conv1 = nn.ConvTranspose1d(1024, 512, kernel_size=9, stride=2, padding=4, output_padding=1)\n",
    "        self.dec_conv2 = nn.ConvTranspose1d(512, 256, kernel_size=9, stride=2, padding=4, output_padding=0)\n",
    "        self.dec_conv3 = nn.ConvTranspose1d(256, 128, kernel_size=9, stride=2, padding=5, output_padding=0)\n",
    "        self.dec_conv4 = nn.ConvTranspose1d(128, 64, kernel_size=9, stride=2, padding=4, output_padding=0)\n",
    "        self.dec_conv5 = nn.ConvTranspose1d(64,32, kernel_size=9, stride=2, padding=4, output_padding=0)\n",
    "        self.dec_conv6 = nn.ConvTranspose1d(32, 16, kernel_size=9, stride=2, padding=4, output_padding=0)\n",
    "        self.dec_conv7 = nn.ConvTranspose1d(16, 8, kernel_size=21, stride=4, padding=9, output_padding=0)\n",
    "        self.dec_conv8 = nn.ConvTranspose1d(8, 1, kernel_size=37, stride=4, padding=22, output_padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.enc_conv1(x)\n",
    "        x = self.enc_conv2(x)\n",
    "        x = self.enc_conv3(x)\n",
    "        x = self.enc_conv4(x)\n",
    "        x = self.enc_conv5(x)\n",
    "        x = self.enc_conv6(x)\n",
    "        x = self.enc_conv7(x)\n",
    "        x = self.enc_conv8(x)\n",
    "        encoded = x\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.dec_conv1(x)\n",
    "        x = self.dec_conv2(x)\n",
    "        x = self.dec_conv3(x)\n",
    "        x = self.dec_conv4(x)\n",
    "        x = self.dec_conv5(x)\n",
    "        x = self.dec_conv6(x)\n",
    "        x = self.dec_conv7(x)\n",
    "        x = self.dec_conv8(x)\n",
    "\n",
    "        x = x[:,:,:160000]\n",
    "        return x, encoded\n",
    "\n",
    "    def compute_loss(self, outputs, ref_signals):\n",
    "        loss = self.loss_fn_1(outputs, ref_signals) + self.loss_fn_2(outputs, ref_signals) + self.loss_fn_3(outputs, ref_signals)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        audio = batch\n",
    "        \n",
    "        output_audio, encoded = self.forward(audio)\n",
    "        # print(outputs.size())\n",
    "\n",
    "        if batch_idx % 64 == 0:\n",
    "            for ii in range(4):\n",
    "                input_signal = audio[ii].cpu().detach().numpy().T\n",
    "                reconstucted_signal = output_audio[ii].cpu().detach().numpy().T\n",
    "                wandb.log({f'audio_input_{ii}': [wandb.Audio(input_signal, caption=\"Input\", sample_rate=32000)]})\n",
    "                wandb.log({f'audio_reconstructed_{ii}': [wandb.Audio(reconstucted_signal, caption=\"Reconstructed\", sample_rate=32000)]})\n",
    "\n",
    "        print(audio.shape)\n",
    "        print(encoded.shape)\n",
    "        print(output_audio.shape)\n",
    "\n",
    "\n",
    "        loss = self.compute_loss(output_audio, audio)         \n",
    "\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define your optimizer and optionally learning rate scheduler here\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.99)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41495611-c201-43dd-8f62-4f76d94ce9b4",
   "metadata": {},
   "source": [
    "# Callbacks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeb52df-9b86-4786-98ff-284d5e71c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveModelEveryNSteps(pl.Callback):\n",
    "    def __init__(self, save_step_frequency=512,):\n",
    "        self.save_step_frequency = save_step_frequency\n",
    "        self.save_path = \"D://Github/timbre-tools-hack//Feature_Extraction//models//AE//\"\n",
    "        os.makedirs(self.save_path , exist_ok=True)\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        if (trainer.global_step + 1) % self.save_step_frequency == 0:\n",
    "            checkpoint_path = os.path.join(self.save_path, f\"step_{trainer.global_step + 1}.ckpt\")\n",
    "            trainer.save_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2864b1-8958-4bb4-8b5d-bb28c974bcc7",
   "metadata": {},
   "source": [
    "# Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd279ac-e5d6-400c-8e59-83341bd30011",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf6730f-59d1-4039-a08e-5482031dafe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(project='TT-AE', log_model='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91245d-8e66-47cc-b78c-d8f0a670bf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data_module = AudioDataModule(all_scenes, batch_size=16, num_workers=0, persistent_workers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511721ef-4c92-44aa-9ec7-b8cb840e8f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=1000,\n",
    "    accelerator=\"gpu\", \n",
    "    devices=-1,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[SaveModelEveryNSteps()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcea643-a238-449f-b0fe-da5f0c83e019",
   "metadata": {},
   "source": [
    "## Actually fit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538322fa-a50b-4a36-8bad-580f974c5294",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, audio_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d953965-18cd-461c-be05-3d34b412f3f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f89468-5d8c-455b-a152-2fcd18f8c519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd8f2cac-fc6b-4069-85d1-cf4145032db4",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
