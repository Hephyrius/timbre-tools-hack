{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T15:27:15.087017Z",
     "start_time": "2024-02-23T15:27:08.678960Z"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------\n",
    "# 03. Corpus-based analysis and synthesis\n",
    "#\n",
    "# This notebook demonstrates how to:\n",
    "#  - extract features from a pre-recorded corpus of audio (here, a large audio file)\n",
    "#  - perform dimensionality reduction\n",
    "#  - transform feature arrays into a pandas dataframe\n",
    "#  - create interactive plots to play back grains in latent feature space\n",
    "#  - use audio-rate modulation to modulate playback of grains\n",
    "#  - use timeline-based sequencing to trigger playback of grains\n",
    "#\n",
    "# TODO: Different segmentation methods\n",
    "#\n",
    "# Requirements:\n",
    "#   pip3 install numpy librosa scikit-learn ipython matplotlib altair pandas anywidget isobar\n",
    "#------------------------------------------------------------------------------------------------\n",
    "\n",
    "import librosa\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import sklearn.cluster\n",
    "import IPython.display\n",
    "import sklearn.decomposition\n",
    "import sklearn.preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Hide numerical warnings and permit large datasets\n",
    "warnings.filterwarnings('ignore')\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "from signalflow import *\n",
    "graph = AudioGraph()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T15:27:15.116668Z",
     "start_time": "2024-02-23T15:27:14.721537Z"
    }
   },
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Load an audio file to analyse and process\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# filename = \"audio/ligeti-atmospheres.wav\"\n",
    "# filename = \"audio/sunkilmoon-truckers-atlas-loop.wav\"\n",
    "# filename = \"/Volumes/T7_Touch/ocd/generation/data_output/staging/rendered_audio/140e6d3e-f923-4341-a20b-cbf38e85f058.flac\"\n",
    "# filename = \"/Volumes/T7_Touch/ocd/generation/data_output/staging/rendered_audio/510319a4-513f-47f0-97c5-da3fb1e62a68.flac\"\n",
    "filename = '/Users/jeffreymiller/Dropbox/Research/PhD/Projects/chord_aggregator/timbre-tools-hack/Data/concatenated_audio/combined_0_3_7_10.flac'\n",
    "\n",
    "buffer = Buffer(filename)\n",
    "IPython.display.Audio(buffer.data, rate=buffer.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = Buffer('/Users/jeffreymiller/Dropbox/Research/PhD/Projects/chord_aggregator/timbre-tools-hack/Data/concatenated_audio/combined_0_2_5_8.flac')\n",
    "# buffer = Buffer('/Users/jeffreymiller/Dropbox/Research/PhD/Projects/chord_aggregator/timbre-tools-hack/Data/concatenated_audio/combined_0_2_5_9.flac')\n",
    "# buffer = Buffer('/Users/jeffreymiller/Dropbox/Research/PhD/Projects/chord_aggregator/timbre-tools-hack/Data/concatenated_audio/combined_0_4_7_11.flac')\n",
    "# buffer = Buffer('/Users/jeffreymiller/Dropbox/Research/PhD/Projects/chord_aggregator/timbre-tools-hack/Data/concatenated_audio/combined_0_1_5_8.flac')\n",
    "# buffer = Buffer('/Users/jeffreymiller/Dropbox/Research/PhD/Projects/chord_aggregator/timbre-tools-hack/Data/concatenated_audio/combined_0_2_7.flac')\n",
    "\n",
    "IPython.display.Audio(buffer.data, rate=buffer.sample_rate)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import random as r\n",
    "from tqdm import tqdm\n",
    "\n",
    "path = \"/Volumes/T7_Touch/ocd/generation/data_output/staging/rendered_audio/\"\n",
    "files = os.listdir(path)\n",
    "r.shuffle(files)\n",
    "fs = []\n",
    "embeddings = []\n",
    "audio_list = []\n",
    "\n",
    "for file in tqdm(files[:1000]):\n",
    "    if \".flac\" in file:\n",
    "        try:\n",
    "            full_path = path + file\n",
    "            audio, _ = torchaudio.load(full_path)\n",
    "            audio_list.append( audio.squeeze(0).numpy())\n",
    "\n",
    "            \n",
    "            fs.append(full_path)\n",
    "        except Exception as e:\n",
    "            print(\"error\", e)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "buffer = np.concatenate(audio_list, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T15:27:23.599066Z",
     "start_time": "2024-02-23T15:27:20.414223Z"
    }
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Set global variables: FFT size, hop size, sample rate\n",
    "#--------------------------------------------------------------------------------\n",
    "fft_size = 16384\n",
    "# fft_size = 4096\n",
    "hop_size = fft_size // 2\n",
    "# sample_rate = buffer.sample_rate\n",
    "sample_rate = 44100\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Extract MFCC, and rescale to zero-mean, unit-variance.\n",
    "# FluCoMa has a nice interactive explainer that gives some intuition:\n",
    "# https://learn.flucoma.org/reference/mfcc/\n",
    "#--------------------------------------------------------------------------------\n",
    "# X = librosa.feature.mfcc(y=buffer, sr=sample_rate, n_fft=fft_size, hop_length=hop_size, n_mfcc=20)\n",
    "X = librosa.feature.mfcc(y=buffer.data[0], sr=sample_rate, n_fft=fft_size, hop_length=hop_size, n_mfcc=20)\n",
    "X = sklearn.preprocessing.scale(X)\n",
    "X = X.T\n",
    "print(\"MFCC coefficient shape: %s\" % str(X.shape))\n",
    "print(np.round(X[:8,:8], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Perform Principal Component Analysis (PCA) to reduce the dimensionality of\n",
    "# each input MFCC frame, and extract various manually-specified features.\n",
    "#--------------------------------------------------------------------------------\n",
    "model = sklearn.decomposition.PCA(n_components=3, whiten=True)\n",
    "model.fit(X)\n",
    "Y = model.transform(X)\n",
    "print(\"Y shape: %s\" % str(Y.shape))\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Create data series, containing per-segment properties which are later needed\n",
    "# for display and playback:\n",
    "#  - ordinal index\n",
    "#  - timestamp (in seconds)\n",
    "#  - duration (the same for every block, as we're using identically-sized blocks)\n",
    "#--------------------------------------------------------------------------------\n",
    "index = np.arange(len(Y))\n",
    "timestamp = index * hop_size / sample_rate\n",
    "duration = fft_size / sample_rate\n",
    "duration_array = np.array([duration] * len(Y))\n",
    "\n",
    "def floats_to_ordinals(floats):\n",
    "    sorted_indices = np.argsort(floats)\n",
    "    positions = np.argsort(sorted_indices)\n",
    "    return positions.tolist()\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Manually extract a few features to add to the data frame:\n",
    "#  - spectral centroid\n",
    "#  - spectral flatness\n",
    "#  - k-means cluster\n",
    "#--------------------------------------------------------------------------------\n",
    "centroid = librosa.feature.spectral_centroid(y=buffer.data[0], sr=44100, n_fft=fft_size, hop_length=hop_size)[0]\n",
    "flatness = librosa.feature.spectral_flatness(y=buffer.data[0], n_fft=fft_size, hop_length=hop_size)[0]\n",
    "flatness = floats_to_ordinals(flatness)\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters=4)\n",
    "labels = kmeans.fit_predict(Y)\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Aggregate all features into a pandas DataFrame, with columns for each feature\n",
    "# and a row for each segment\n",
    "#--------------------------------------------------------------------------------\n",
    "df = pd.DataFrame({\n",
    "    \"a\": Y[:,0],\n",
    "    \"b\": Y[:,1],\n",
    "    \"c\": Y[:,2],\n",
    "    \"centroid\": centroid,\n",
    "    \"flatness\": flatness,\n",
    "    \"index\": index,\n",
    "    \"timestamp\": timestamp,\n",
    "    \"duration\": duration_array,\n",
    "    \"cluster\": labels,\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer = Buffer(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T15:27:31.343257Z",
     "start_time": "2024-02-23T15:27:31.211220Z"
    }
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Altair scatter plot displaying each grain's location within feature space,\n",
    "# with a point-based selector to trigger playback of grains\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "chart = alt.Chart(df, width=800, height=500)\n",
    "chart = chart.mark_circle(size=40)\n",
    "chart = chart.encode(x=alt.X(\"a\"),\n",
    "                     y=alt.Y(\"b\"),\n",
    "                     color=alt.Color('timestamp').scale(scheme=\"plasma\"),\n",
    "                     tooltip=[\"index\", \"timestamp\"])\n",
    "\n",
    "selector = alt.selection_point(name=\"point\",\n",
    "                               on='mouseover',\n",
    "                               nearest=True)\n",
    "chart = chart.add_params(selector)\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# SegmentedGranulator plays back a segment of the input file when triggered\n",
    "#--------------------------------------------------------------------------------\n",
    "granulator = SegmentedGranulator(buffer,\n",
    "                                 df.timestamp,\n",
    "                                 df.duration)\n",
    "# granulator.set_buffer(\"envelope\", EnvelopeBuffer(\"hanning\"))\n",
    "granulator.set_buffer(\"envelope\", EnvelopeBuffer(\"linear-decay\"))\n",
    "#granulator.set_buffer(\"envelope\", EnvelopeBuffer(\"triangle\"))\n",
    "attenuated = granulator * 0.75\n",
    "attenuated.play()\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Altair callback to trigger a grain on hover\n",
    "#--------------------------------------------------------------------------------\n",
    "def on_select(change):\n",
    "    value = change[\"new\"].value\n",
    "    if value:\n",
    "        index = value[0]\n",
    "        granulator.trigger(\"trigger\", index)\n",
    "        granulator.index = index\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Add Jupyter interactivity\n",
    "#--------------------------------------------------------------------------------\n",
    "jchart = alt.JupyterChart(chart)\n",
    "jchart.selections.observe(on_select, [\"point\"])\n",
    "jchart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# This example demonstrates playing features in real-time, using SignalFlow\n",
    "# LFO objects to modulate the X/Y position in feature space.\n",
    "#\n",
    "# The NearestNeighbour node performs a search for the nearest datapoint to\n",
    "# the specified `target` coordinate.\n",
    "# \n",
    "# The signalflow_analysis library contains the AudioFeatureBuffer object,\n",
    "# which encodes N-dimensional frame-wise feature properties. AudioFeatureBuffer\n",
    "# is a subclass of the generic \"Buffer\" signalflow class.\n",
    "#\n",
    "#  - Each channel of the buffer corresponds to a feature\n",
    "#  - Each sample in the buffer corresponds to a segment (block) of the input\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "from signalflow_analysis import *\n",
    "\n",
    "xpos = SineLFO(0.5, -1, 1)\n",
    "ypos = SineLFO(0.71, -1, 1)\n",
    "feature_buffer = AudioFeatureBuffer([df.a, df.b])\n",
    "nearest_index = NearestNeighbour(feature_buffer, target=[xpos, ypos])\n",
    "player = SegmentedGranulator(buffer=buffer,\n",
    "                             onset_times=df.timestamp,\n",
    "                             durations=df.duration,\n",
    "                             index=nearest_index,\n",
    "                             clock=RandomImpulse(5))\n",
    "attenuated = player * 0.5\n",
    "attenuated.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Modulate playback parameters interactively.\n",
    "#--------------------------------------------------------------------------------\n",
    "player.clock = RandomImpulse(20)\n",
    "nearest_index.target = [xpos, ypos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Example using the isobar sister library for sequencing note events.\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "from isobar import *\n",
    "\n",
    "class NearestNeighbourTrigger (Patch):\n",
    "    #--------------------------------------------------------------------------------\n",
    "    # This patch encapsulates a granulator that plays back grains selected via\n",
    "    # a feature buffer, when triggered.\n",
    "    #--------------------------------------------------------------------------------\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        x = self.add_input(\"centroid\")\n",
    "        y = self.add_input(\"flatness\")\n",
    "        feature_buffer = AudioFeatureBuffer([df.centroid, df.flatness])\n",
    "        nn = NearestNeighbour(feature_buffer, [x, y])\n",
    "        granulator = SegmentedGranulator(buffer,\n",
    "                                         df.timestamp,\n",
    "                                         df.duration * 0.2,\n",
    "                                         index=nn)\n",
    "        # how to create envelope buffer from segments / shape?\n",
    "        granulator.set_buffer(\"envelope\", EnvelopeBuffer(\"linear-decay\"))\n",
    "        delay = AllpassDelay(granulator, 0.1, feedback=0.9)\n",
    "        output = granulator + delay * 0.5\n",
    "        self.set_output(output)\n",
    "        self.set_trigger_node(granulator)\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Create the patch and connect it to the graph.\n",
    "#--------------------------------------------------------------------------------\n",
    "nnpatch = NearestNeighbourTrigger()\n",
    "nnpatch.play()\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Create a 150bpm timeline.\n",
    "#--------------------------------------------------------------------------------\n",
    "timeline = Timeline(100, output_device=SignalFlowOutputDevice(graph=graph))\n",
    "timeline.background()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline.schedule({\n",
    "    \"patch\": nnpatch,\n",
    "    \"type\": \"trigger\",\n",
    "    \"duration\": 0.5,\n",
    "    \"params\": {\n",
    "        # \"feature_n\": PSequence([]) + PWhite(),\n",
    "        # \"centroid\": PSequence([9000, 200, 9000, 100, 7000]) + PWhite(-200, 1000),\n",
    "        # \"flatness\": PSequence([100, 3000, 2000, 3000]) + PWhite(-500, 500)\n",
    "        \"pca_a\": PSequence([]) + PWhite(),\n",
    "        \"pca_b\": PSequence([]) + PWhite(),\n",
    "        \"pca_c\": PSequence([]) + PWhite(),\n",
    "    }\n",
    "}, name=\"track\", replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T15:12:59.368125Z",
     "start_time": "2024-02-23T15:12:59.341525Z"
    }
   },
   "outputs": [],
   "source": [
    " timeline.clear()\n",
    "graph.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
